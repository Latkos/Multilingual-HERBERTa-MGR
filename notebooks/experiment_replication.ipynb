{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-27T08:42:12.787262500Z",
          "start_time": "2023-11-27T08:42:12.773261600Z"
        },
        "id": "7MAQTQJZW89n"
      },
      "outputs": [],
      "source": [
        "# This notebook allows replicating the experiments described in \"Experiments\" section of the thesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-13T19:03:42.207735300Z",
          "start_time": "2023-12-13T19:03:42.169995300Z"
        },
        "id": "hnLhUMEaW89o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-26T20:18:26.630403400Z",
          "start_time": "2023-11-26T20:18:26.615116300Z"
        },
        "pycharm": {
          "is_executing": true
        },
        "scrolled": true,
        "id": "-EdIVB1AW89p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from experiments.enhancing_with_ner import test_enhancing_text_used_to_train_re\n",
        "from experiments.hyperparameters import optuna_hp_space, optuna_hp_space_scientific\n",
        "from experiments.model_size import test_ner_quality_depending_on_dataset_size, test_re_quality_depending_on_dataset_size\n",
        "from named_entity.named_entity_model import NamedEntityModel\n",
        "from relations.relations_model import RelationsModel\n",
        "from utils.evaluation import evaluate_with_division_between_column\n",
        "from utils.optuna_reader import read_optuna_logs\n",
        "from utils.prediction import train_re_on_ner, predict_joint_models\n",
        "from utils.preprocessing import filter_out_wrong_data\n",
        "from utils.overlap import create_full_matrix\n",
        "from utils.enhancement import enhance_with_nothing, enhance_with_entity, enhance_with_brackets, \\\n",
        "    enhance_with_entity_differentiated, enhance_with_special_characters, enhance_entities_only\n",
        "from experiments.hyperparameters import test_hyperparameters_impact\n",
        "from experiments.sample_languages import run_experiments_linguistic, perform_four_variations_linguistic, train_and_evaluate_on_language_subsets\n",
        "from utils.overlap import remove_overlapping_entities\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-26T20:18:28.713816700Z",
          "start_time": "2023-11-26T20:18:28.701312Z"
        },
        "id": "TGsdvRvlW89q"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 10000)\n",
        "pd.set_option('display.max_colwidth', 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkrdsGodW89q"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(mode=\"disabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhaKQYTHW89q"
      },
      "source": [
        "# Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-26T20:17:06.456566700Z",
          "start_time": "2023-11-26T20:17:06.441484Z"
        },
        "id": "gwFNG0voW89r"
      },
      "outputs": [],
      "source": [
        "NER_BROAD_FILE_PATH='results/hyperparameter_optimization_ner_broad.txt'\n",
        "RE_BROAD_FILE_PATH='results/hyperparameter_optimization_re_broad.txt'\n",
        "NER_SCIENTIFIC_FILE_PATH='results/hyperparameter_optimization_ner_scientific.txt'\n",
        "RE_SCIENTIFIC_FILE_PATH='results/hyperparameter_optimization_re_scientific.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-26T20:17:06.829253900Z",
          "start_time": "2023-11-26T20:17:06.457456700Z"
        },
        "scrolled": true,
        "id": "m-DS00GIW89s"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "train_df = pd.read_csv(\"merged_train.tsv\", sep=\"\\t\")\n",
        "test_df = pd.read_csv(\"merged_test.tsv\", sep=\"\\t\")\n",
        "train_df=train_df.sample(frac=0.02,random_state=42)\n",
        "train_df = filter_out_wrong_data(train_df)\n",
        "test_df = filter_out_wrong_data(test_df)\n",
        "ner_model=NamedEntityModel()\n",
        "re_model=RelationsModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-FNjiW2W89s"
      },
      "source": [
        "## Broad search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-26T20:17:06.835185900Z",
          "start_time": "2023-11-26T20:17:06.831247400Z"
        },
        "id": "Y0b0ZihjW89s"
      },
      "outputs": [],
      "source": [
        "%%capture captured\n",
        "ner_model.perform_hyperparameter_search(space=optuna_hp_space,train_df=train_df, study_name=\"ner_hyperparameter_search_broad\")\n",
        "with open(NER_BROAD_FILE_PATH, 'w') as f:\n",
        "    f.write(captured.stdout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.832248700Z"
        },
        "id": "B4W4OeYFW89t"
      },
      "outputs": [],
      "source": [
        "%%capture captured\n",
        "re_model.perform_hyperparameter_search(space=optuna_hp_space,train_df=train_df, study_name=\"re_hyperparameter_search_broad\")\n",
        "with open(RE_BROAD_FILE_PATH, 'w') as f:\n",
        "    f.write(captured.stdout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zFjyyVsW89t"
      },
      "source": [
        "## Scientific-based search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.833170200Z"
        },
        "id": "dTlS54ZaW89u"
      },
      "outputs": [],
      "source": [
        "%%capture captured\n",
        "ner_model.perform_hyperparameter_search(space=optuna_hp_space_scientific,train_df=train_df, study_name=\"ner_hyperparameter_search_scientific\")\n",
        "with open(NER_SCIENTIFIC_FILE_PATH, 'w') as f:\n",
        "    f.write(captured.stdout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.834213800Z"
        },
        "id": "wGw07wsvW89u"
      },
      "outputs": [],
      "source": [
        "%%capture captured\n",
        "re_model.perform_hyperparameter_search(space=optuna_hp_space_scientific,train_df=train_df, study_name=\"re_hyperparameter_search_scientific\")\n",
        "with open(RE_SCIENTIFIC_FILE_PATH, 'w') as f:\n",
        "    f.write(captured.stdout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43cjE5xOW89u"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.835185900Z"
        },
        "id": "QNAiDj-mW89u"
      },
      "outputs": [],
      "source": [
        "def plot_histogram(df, column, yaxis_range=None):\n",
        "    fig = px.histogram(df, x=column, y=\"metric\", title=f\"{column} impact on metric\", histfunc='avg')\n",
        "    # Check if a y-axis range is provided, and if so, set it\n",
        "    if yaxis_range:\n",
        "        fig.update_layout(yaxis=dict(range=yaxis_range))\n",
        "    fig.show()\n",
        "\n",
        "def plot_scatter(df, column, yaxis_range=None):\n",
        "    fig = px.scatter(df, x=column, y=\"metric\", title=f\"{column} impact on metric\")\n",
        "    # Check if a y-axis range is provided, and if so, set it\n",
        "    if yaxis_range:\n",
        "        fig.update_layout(yaxis=dict(range=yaxis_range))\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def analyze_optuna_results(\n",
        "    file_path,\n",
        "    all_histograms=False,\n",
        "    exclude_columns=[\"metric\", \"trial_number\", \"trial_runtime\"],\n",
        "    calculate_correlation=True,\n",
        "    yaxis_range=None\n",
        "):\n",
        "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "    print(f\"Analyzing: {file_name}\")\n",
        "    df = read_optuna_logs(file_path)\n",
        "    for column in df.columns:\n",
        "        if column not in exclude_columns:\n",
        "            if all_histograms or df[column].dtype == 'int64':\n",
        "                plot_histogram(df, column, yaxis_range)\n",
        "            else:\n",
        "                plot_scatter(df, column, yaxis_range)\n",
        "    plot_scatter(df, \"trial_number\", yaxis_range)\n",
        "    if calculate_correlation:\n",
        "        correlation_data = []\n",
        "        for column in df.columns:\n",
        "            if column not in exclude_columns:\n",
        "                corr = df[column].corr(df['metric'], method='spearman')\n",
        "                corr = round(corr, 2)\n",
        "                correlation_data.append({'Parameter': column, 'Spearman Correlation': corr})\n",
        "        correlation_df = pd.DataFrame(correlation_data)\n",
        "        print(\"Spearman Correlations with Metric:\")\n",
        "        display(correlation_df.sort_values(by='Spearman Correlation', ascending=False))\n",
        "    display(df.sort_values(by=\"metric\", ascending=False))\n",
        "\n",
        "def compare_two_studies(exploratory_file_path, literature_based_file_path, yaxis_range=None):\n",
        "    df1 = read_optuna_logs(exploratory_file_path)\n",
        "    df2 = read_optuna_logs(literature_based_file_path)\n",
        "    df1['study'] = \"exploratory\"\n",
        "    df2['study'] = \"literature-based\"\n",
        "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "    fig = px.scatter(combined_df, x='trial_number', y='metric', color='study', title=\"Trial number vs F1\")\n",
        "    fig.update_layout(showlegend=False)\n",
        "    if yaxis_range:\n",
        "        fig.update_layout(yaxis=dict(range=yaxis_range))\n",
        "    fig.update_xaxes(tickvals=list(range(0,50)))\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.836272100Z"
        },
        "scrolled": true,
        "id": "-PgRw-CNW89v"
      },
      "outputs": [],
      "source": [
        "analyze_optuna_results(file_path=NER_BROAD_FILE_PATH, yaxis_range=[0.75,0.85])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.836858700Z"
        },
        "id": "BxEvJt4bW89v"
      },
      "outputs": [],
      "source": [
        "analyze_optuna_results(file_path=RE_BROAD_FILE_PATH, yaxis_range=[0.9,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.837910900Z"
        },
        "id": "rNNXI3E8W89v"
      },
      "outputs": [],
      "source": [
        "analyze_optuna_results(file_path=NER_SCIENTIFIC_FILE_PATH, yaxis_range=[0.75,0.85], all_histograms=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-26T20:17:06.848108600Z",
          "start_time": "2023-11-26T20:17:06.838939300Z"
        },
        "scrolled": true,
        "id": "j3KlyQ1tW89v"
      },
      "outputs": [],
      "source": [
        "analyze_optuna_results(file_path=RE_SCIENTIFIC_FILE_PATH, yaxis_range=[0.9,1], all_histograms=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.839919900Z"
        },
        "id": "OKJ7bXkvW89w"
      },
      "outputs": [],
      "source": [
        "compare_two_studies(NER_BROAD_FILE_PATH, NER_SCIENTIFIC_FILE_PATH, [0.75,0.85])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.840940200Z"
        },
        "id": "OenQSzSmW89w"
      },
      "outputs": [],
      "source": [
        "compare_two_studies(RE_BROAD_FILE_PATH, RE_SCIENTIFIC_FILE_PATH, [0.9,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOeJAq-IW89w"
      },
      "source": [
        "## Freezing hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.841943100Z"
        },
        "id": "KLLArTOJW89w"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "train_df = pd.read_csv(\"data/merged_train.tsv\", sep=\"\\t\")\n",
        "test_df = pd.read_csv(\"data/merged_test.tsv\", sep=\"\\t\")\n",
        "train_df=train_df.sample(frac=0.1,random_state=42)\n",
        "train_df = filter_out_wrong_data(train_df)\n",
        "test_df = filter_out_wrong_data(test_df)\n",
        "ner_model=NamedEntityModel('models/freezing_hyperparameters')\n",
        "re_model=RelationsModel('models/freezing_re_hyperparameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.842923900Z"
        },
        "id": "YAexbBKbW89x"
      },
      "outputs": [],
      "source": [
        "ner_results_freezing=test_hyperparameters_impact(ner_model,train_df,test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZloJ6bHW89x"
      },
      "outputs": [],
      "source": [
        "ner_results_freezing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foeCsoYjW89x"
      },
      "outputs": [],
      "source": [
        "re_results_freezing=test_hyperparameters_impact(ner_model,train_df,test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyeNdSwOW89y"
      },
      "outputs": [],
      "source": [
        "re_results_freezing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGsi-LhbW89y"
      },
      "source": [
        "# Dataset size impact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.842923900Z"
        },
        "id": "3agYKPBFW89y"
      },
      "outputs": [],
      "source": [
        "SIZES=[\n",
        "    100,500,1000,2000,5000,10000,20000,50000,100000,200000,\n",
        "    # 300000,400000,500000\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.843940700Z"
        },
        "id": "fVq_hpMrW89z"
      },
      "outputs": [],
      "source": [
        "total_results_ner={}\n",
        "total_results_re={}\n",
        "total_results_re_no_tags={}\n",
        "for dataset in [\n",
        "    'merged',\n",
        "    # 'en-full',\n",
        "    # 'pl'\n",
        "]:\n",
        "    print(f\"DATASET: {dataset}\")\n",
        "    train_df = pd.read_csv(f\"data/{dataset}_corpora_train.tsv\", sep=\"\\t\")\n",
        "    test_df = pd.read_csv(f\"data/{dataset}_corpora_test.tsv\", sep=\"\\t\")\n",
        "    train_df = filter_out_wrong_data(train_df)\n",
        "    test_df = filter_out_wrong_data(test_df)\n",
        "    ner_model=NamedEntityModel()\n",
        "    # re_model=RelationsModel()\n",
        "    ner_results=test_ner_quality_depending_on_dataset_size(model=ner_model, train_df=train_df, test_df=test_df, sizes=SIZES, random_state=42)\n",
        "    total_results_ner[dataset]=ner_results\n",
        "    # re_results=test_re_quality_depending_on_dataset_size(model=re_model, train_df=train_df, test_df=test_df, sizes=SIZES, random_state=42, remove_tags=False)\n",
        "    # total_results_re[dataset]=re_results\n",
        "    # re_no_tags_results=test_re_quality_depending_on_dataset_size(model=re_model, train_df=train_df, test_df=test_df, sizes=SIZES, random_state=42, enhancement_func=enhance_with_nothing,\n",
        "    #                                                              remove_tags=True)\n",
        "    # total_results_re_no_tags[dataset]=re_no_tags_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.845488200Z"
        },
        "id": "htOAE0RUW89z"
      },
      "outputs": [],
      "source": [
        "total_results_ner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apLN17BlW89z"
      },
      "outputs": [],
      "source": [
        "total_results_ner_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.845488200Z"
        },
        "id": "SO3OhBBKW890"
      },
      "outputs": [],
      "source": [
        "total_results_re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.847043500Z"
        },
        "id": "AjS2mkYKW890"
      },
      "outputs": [],
      "source": [
        "total_results_re_no_tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1P0y8NPW890"
      },
      "source": [
        "# Training base NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-26T20:17:06.896681300Z",
          "start_time": "2023-11-26T20:17:06.848108600Z"
        },
        "id": "zUeeRLSRW890"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "train_df = pd.read_csv(\"data/merged_train.tsv\", sep=\"\\t\")\n",
        "test_df = pd.read_csv(\"data/merged_test.tsv\", sep=\"\\t\")\n",
        "train_df=train_df.sample(frac=0.5,random_state=42)\n",
        "train_df = filter_out_wrong_data(train_df)\n",
        "test_df = filter_out_wrong_data(test_df)\n",
        "ner_model=NamedEntityModel('models/default_50_ner')\n",
        "ner_model.train(train_df=train_df)\n",
        "ner_model.evaluate(df=test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYXnlZ73W890"
      },
      "source": [
        "# Joining The Models Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.849129700Z"
        },
        "id": "BXWOxh3RW890"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "train_df = pd.read_csv(\"merged_train.tsv\", sep=\"\\t\")\n",
        "test_df = pd.read_csv(\"merged_test.tsv\", sep=\"\\t\")\n",
        "train_df=train_df.sample(frac=1,random_state=42)\n",
        "train_df = filter_out_wrong_data(train_df)\n",
        "test_df = filter_out_wrong_data(test_df)\n",
        "ner_model=NamedEntityModel('models/base_ner')\n",
        "re_model=RelationsModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.850092900Z"
        },
        "scrolled": true,
        "id": "S4UEH1iBW8-F"
      },
      "outputs": [],
      "source": [
        "# read parameter needs to be set to False if predicting for the first time (and lacking NER prediction results for a given dataset subset)\n",
        "test_enhancing_text_used_to_train_re(train_df, test_df, ner_model, re_model, results_file='results_base_ner.pkl', read=True, train_ner=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLKXQh_SW8-F"
      },
      "source": [
        "# Model Variant Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.850092900Z"
        },
        "id": "Ydor-4bjW8-F"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "train_df = pd.read_csv(\"merged_train.tsv\", sep=\"\\t\")\n",
        "test_df = pd.read_csv(\"merged_test.tsv\", sep=\"\\t\")\n",
        "train_df=train_df.sample(frac=1,random_state=42)\n",
        "train_df = filter_out_wrong_data(train_df)\n",
        "test_df = filter_out_wrong_data(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU5XXeycW8-F"
      },
      "source": [
        "## DistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.852069200Z"
        },
        "id": "rK5DilfSW8-F"
      },
      "outputs": [],
      "source": [
        "ner_model=NamedEntityModel(model_type='distilbert-base-multilingual-cased',model_path='models/distilbert_ner')\n",
        "re_model=RelationsModel(model_type='distilbert-base-multilingual-cased',model_path='models/distillbert_re')\n",
        "# ner_model.train(train_df=train_df)\n",
        "# distilbert_results_ner=ner_model.evaluate(df=test_df)\n",
        "distilbert_results_re=train_re_on_ner(ner_model=ner_model, re_model=re_model, train_df=train_df, test_df=test_df, enhancement_func=enhance_with_special_characters, results_file='results_distilbert_ner.pkl', read=True)\n",
        "# After the first call, the prediction results are surely saved, so we can set read to Tru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.852069200Z"
        },
        "id": "20OyY9F0W8-F"
      },
      "outputs": [],
      "source": [
        "distilbert_results_re_enhance_with_nothing=train_re_on_ner(ner_model=ner_model, re_model=re_model, train_df=train_df, test_df=test_df, enhancement_func=enhance_with_nothing, results_file='results_distilbert_ner.pkl', read=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JO-SAYHW8-G"
      },
      "source": [
        "## XLMRoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.853064600Z"
        },
        "id": "KrzhXIlXW8-G"
      },
      "outputs": [],
      "source": [
        "ner_model=NamedEntityModel(model_type='xlm-roberta-base',model_path='models/xlmroberta_ner')\n",
        "re_model=RelationsModel(model_type='xlm-roberta-base',model_path='models/xlmroberta_re')\n",
        "# ner_model.train(train_df=train_df)\n",
        "# xlmroberta_results_ner=ner_model.evaluate(df=test_df)\n",
        "xlmroberta_results_re=train_re_on_ner(ner_model=ner_model, re_model=re_model, train_df=train_df, test_df=test_df, enhancement_func=enhance_with_special_characters, results_file='results_xlm_roberta.pkl', read=True)\n",
        "# After the first call, the prediction results are surely saved, so we can set read to True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.854061900Z"
        },
        "id": "9CCdLVlIW8-G"
      },
      "outputs": [],
      "source": [
        "xlmroberta_results_re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.855112100Z"
        },
        "id": "QLWq5EMRW8-G"
      },
      "outputs": [],
      "source": [
        "ner_model=NamedEntityModel(model_type='xlm-roberta-base',model_path='models/xlmroberta_ner')\n",
        "re_model=RelationsModel(model_type='xlm-roberta-base',model_path='models/xlmroberta_re')\n",
        "# ner_model.train(train_df=train_df)\n",
        "# xlmroberta_results_ner=ner_model.evaluate(df=test_df)\n",
        "xlmroberta_results_re_no_tags=train_re_on_ner(ner_model=ner_model, re_model=re_model, train_df=train_df, test_df=test_df, enhancement_func=enhance_with_nothing, results_file='results_xlm_roberta.pkl', read=True)\n",
        "# After the first call, the prediction results are surely saved, so we can set read to True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.856692800Z"
        },
        "id": "w-3DoDqbW8-G"
      },
      "outputs": [],
      "source": [
        "xlmroberta_results_re_no_tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVOyfk13W8-G"
      },
      "source": [
        "# Prediction Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.856692800Z"
        },
        "id": "EHQrwaycW8-H"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "train_df = pd.read_csv(\"merged_train.tsv\", sep=\"\\t\")\n",
        "test_df = pd.read_csv(\"merged_test.tsv\", sep=\"\\t\")\n",
        "train_df=train_df.sample(frac=1,random_state=42)\n",
        "train_df = filter_out_wrong_data(train_df)\n",
        "test_df = filter_out_wrong_data(test_df)\n",
        "ner_model=NamedEntityModel(model_path='models/base_ner')\n",
        "re_model=RelationsModel(model_path='models/re_entity_with_special_characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44nJa83SW8-J"
      },
      "source": [
        "## Model predictions detailed dataframe generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1vYfTikbW8-J"
      },
      "outputs": [],
      "source": [
        "prediction_results=predict_joint_models(test_df, ner_model, re_model, enhance_function=enhance_with_special_characters)\n",
        "prediction_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ic87kthiW8-K"
      },
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "    return ''.join(char.lower() for char in text if char.isalnum()).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80hkg1BlW8-K"
      },
      "outputs": [],
      "source": [
        "prediction_results['entity_1_norm'] = prediction_results['entity_1'].apply(normalize_text)\n",
        "prediction_results['predicted_entity_1_norm'] = prediction_results['predicted_entity_1'].apply(normalize_text)\n",
        "prediction_results['entity_2_norm'] = prediction_results['entity_2'].apply(normalize_text)\n",
        "prediction_results['predicted_entity_2_norm'] = prediction_results['predicted_entity_2'].apply(normalize_text)\n",
        "wrong_ner_results = prediction_results[\n",
        "    (prediction_results['entity_1_norm'] != prediction_results['predicted_entity_1_norm']) |\n",
        "    (prediction_results['entity_2_norm'] != prediction_results['predicted_entity_2_norm'])\n",
        "]\n",
        "wrong_re_results = prediction_results[prediction_results['label'] != prediction_results['predicted_label']]\n",
        "print(len(wrong_ner_results))\n",
        "print(len(wrong_re_results))\n",
        "wrong_ner_results.to_csv('results/wrong_ner_results.csv', index=False)\n",
        "wrong_re_results.to_csv('results/wrong_re_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihfLLhp-W8-K"
      },
      "outputs": [],
      "source": [
        "len(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwusN9BoW8-K"
      },
      "source": [
        "# Entity1 vs Entity2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0otF-UXGW8-K"
      },
      "outputs": [],
      "source": [
        "ner_results=ner_model.evaluate(df=test_df)\n",
        "print(f\"Entity 1 F1: {ner_results['eval_Entity1_f1']}\")\n",
        "print(f\"Entity 1 F1: {ner_results['eval_Entity2_f1']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqWLowTTW8-L"
      },
      "source": [
        "## F1 per relation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkVhTZ5gW8-L"
      },
      "outputs": [],
      "source": [
        "f1_per_relation_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.858703Z"
        },
        "id": "4gTm-iqUW8-L"
      },
      "outputs": [],
      "source": [
        "f1_per_relation_df=evaluate_with_division_between_column(model=re_model, test_df=test_df, column_name=\"label\")\n",
        "relation_counts = train_df.groupby('label').size().reset_index(name='num_examples')\n",
        "combined_df = pd.merge(f1_per_relation_df, relation_counts, on=\"label\")\n",
        "display(combined_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.858703Z"
        },
        "id": "YChHPfzKW8-L"
      },
      "outputs": [],
      "source": [
        "correlation = combined_df['num_examples'].corr(combined_df['f1'], method='spearman')\n",
        "print(f\"Correlation between number of examples and F1 score: {correlation:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGRvX51oW8-L"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(combined_df, x='num_examples', y='f1',\n",
        "                 title=\"Dependency of relation's F1 score on number of examples\",\n",
        "                height=600, labels={\"num_examples\": \"Number of examples\", \"f1\":\"f1\"}, text=\"label\"\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.859769500Z"
        },
        "id": "C5sUtXVFW8-L"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import random\n",
        "from itertools import cycle\n",
        "\n",
        "positions = ['top left', 'top center', 'top right', 'middle right', 'bottom right', 'bottom center', 'bottom left', 'middle left']\n",
        "cycled_list = cycle(positions)\n",
        "\n",
        "def update_point(trace, points, selector):\n",
        "    p = list(scatter.textposition)  # get the current location assignments\n",
        "    for i in points.point_inds:  # all selected point indeces\n",
        "        p[i] = next(cycled_list)  # replace corresponding list item by new position\n",
        "        with fig.batch_update():\n",
        "            scatter.textposition = p\n",
        "\n",
        "def random_text_position(x):\n",
        "    positions = ['top left', 'top center', 'top right', 'middle left', 'middle right', 'bottom left', 'bottom center', 'bottom right']  # you can add more: left center ...\n",
        "    return [random.choice(positions) for i in range(len(x))]\n",
        "\n",
        "fig = go.FigureWidget()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=combined_df['num_examples'],\n",
        "    y=combined_df['f1'],\n",
        "    mode=\"markers+text\",\n",
        "    name=\"Markers and Text\",\n",
        "    text=combined_df['label'],\n",
        "    textposition=random_text_position(combined_df['label']),\n",
        "))\n",
        "\n",
        "scatter = fig.data[0]\n",
        "\n",
        "scatter.on_click(update_point)\n",
        "fig.update_layout(width=1000, height=1000)\n",
        "\n",
        "fig\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j4H92iiW8-L"
      },
      "source": [
        "## F1 per language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.860775500Z"
        },
        "id": "JIgf-nXgW8-M"
      },
      "outputs": [],
      "source": [
        "f1_per_language_ner=evaluate_with_division_between_column(model=ner_model, test_df=test_df, column_name=\"lang\")\n",
        "display(f1_per_language_ner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3sBlHMPW8-M"
      },
      "outputs": [],
      "source": [
        "f1_per_language_ner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.861765500Z"
        },
        "id": "jfzDTYSHW8-M"
      },
      "outputs": [],
      "source": [
        "f1_per_language_re=evaluate_with_division_between_column(model=re_model, test_df=test_df, column_name=\"lang\")\n",
        "display(f1_per_language_re)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-XhYh7SW8-M"
      },
      "outputs": [],
      "source": [
        "f1_per_language_re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq81m11eW8-M"
      },
      "source": [
        "# Linguistic Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.863767900Z"
        },
        "id": "TKsfA7huW8-M"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "ner_model=NamedEntityModel()\n",
        "re_model=RelationsModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vxHuV9U5W8-N"
      },
      "outputs": [],
      "source": [
        "# Language family\n",
        "language_family_results=[]\n",
        "language_family_results.append(perform_four_variations_linguistic(\n",
        "    [\"pl\"],\n",
        "    [\"pt\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=5000,\n",
        "    downsample_main=500,\n",
        "    train_monolingual=False,\n",
        "    number_of_runs=5\n",
        "))\n",
        "\n",
        "language_family_results.append(perform_four_variations_linguistic(\n",
        "    [\"es\"],\n",
        "    [\"pt\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=5000,\n",
        "    downsample_main=500,\n",
        "    train_monolingual=False,\n",
        "    number_of_runs=5\n",
        "))\n",
        "\n",
        "language_family_results.append(perform_four_variations_linguistic(\n",
        "    [\"pl\"],\n",
        "    [\"ru\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=5000,\n",
        "    downsample_main=500,\n",
        "    train_monolingual=False,\n",
        "    number_of_runs=5\n",
        "))\n",
        "\n",
        "language_family_results.append(perform_four_variations_linguistic(\n",
        "    [\"ar\"],\n",
        "    [\"ru\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=5000,\n",
        "    downsample_main=500,\n",
        "    train_monolingual=False,\n",
        "    number_of_runs=5\n",
        "))\n",
        "display(pd.DataFrame(language_family_results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIZEZ6LNW8-N"
      },
      "outputs": [],
      "source": [
        "# SVO\n",
        "svo_results=[]\n",
        "svo_results.append(perform_four_variations_linguistic(\n",
        "    [\"ko\"],\n",
        "    [\"fa\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=5000,\n",
        "    downsample_main=500,\n",
        "    number_of_runs=5\n",
        "))\n",
        "svo_results.append(perform_four_variations_linguistic(\n",
        "    [\"it\"],\n",
        "    [\"fa\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=5000,\n",
        "    downsample_main=500,\n",
        "    train_monolingual=False,\n",
        "    number_of_runs=5\n",
        "))\n",
        "svo_results.append(perform_four_variations_linguistic(\n",
        "    [\"pl\"],\n",
        "    [\"fa\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=5000,\n",
        "    downsample_main=500,\n",
        "    train_monolingual=False,\n",
        "    number_of_runs=5\n",
        "))\n",
        "display(pd.DataFrame(svo_results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfnJtBqHW8-N"
      },
      "outputs": [],
      "source": [
        "# SVO but the other way round\n",
        "svo_other_results=[]\n",
        "svo_other_results.append(perform_four_variations_linguistic(\n",
        "    [\"fa\"],\n",
        "    [\"pl\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=2000,\n",
        "    downsample_main=200,\n",
        "    number_of_runs=5\n",
        ")\n",
        "svo_other_results.append(perform_four_variations_linguistic(\n",
        "    [\"ko\"],\n",
        "    [\"pl\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=2000,\n",
        "    downsample_main=200,\n",
        "    number_of_runs=5\n",
        "))\n",
        "svo_other_results.append(perform_four_variations_linguistic(\n",
        "    [\"it\"],\n",
        "    [\"pl\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=2000,\n",
        "    downsample_main=200,\n",
        "    number_of_runs=5\n",
        "))\n",
        "display(pd.DataFrame(svo_other_results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikjg3QMgW8-O"
      },
      "outputs": [],
      "source": [
        "# Cross-script\n",
        "cross_script_result=[]\n",
        "cross_script_result.append(perform_four_variations_linguistic(\n",
        "    [\"fr\"],\n",
        "    [\"nl\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=5000,\n",
        "    downsample_main=500,\n",
        "    number_of_runs=5\n",
        "))\n",
        "cross_script_result.append(perform_four_variations_linguistic(\n",
        "    [\"ru\"],\n",
        "    [\"nl\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=5000,\n",
        "    downsample_main=500,\n",
        "    train_monolingual=False,\n",
        "    number_of_runs=5\n",
        "))\n",
        "display(pd.DataFrame(cross_script_result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LFVJuFbW8-O"
      },
      "outputs": [],
      "source": [
        "# Simple vs complex language\n",
        "simple_language_results=[]\n",
        "simple_language_results.append(perform_four_variations_linguistic(\n",
        "    [\"pl\"],\n",
        "    [\"es\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=5000,\n",
        "    downsample_main=500,\n",
        "    number_of_runs=5\n",
        "))\n",
        "\n",
        "simple_language_results.append(perform_four_variations_linguistic(\n",
        "    [\"nl\"],\n",
        "    [\"es\"],\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type=\"micro\",\n",
        "    downsample_number=5000,\n",
        "    downsample_main=500,\n",
        "    number_of_runs=5\n",
        "))\n",
        "display(pd.DataFrame(simple_language_results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwjZhfd4W8-O"
      },
      "outputs": [],
      "source": [
        "simple_language_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCfXtGx3W8-O"
      },
      "outputs": [],
      "source": [
        "train_df=pd.read_csv('data/fr_corpora_train.tsv',sep='\\t')\n",
        "test_df=pd.read_csv('data/it_corpora_test.tsv',sep='\\t')\n",
        "len(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oea2KfZ4W8-O"
      },
      "outputs": [],
      "source": [
        "train_df=remove_overlapping_entities(train_df,test_df)\n",
        "len(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U31vhUKlW8-O"
      },
      "outputs": [],
      "source": [
        "train_and_evaluate_on_language_subsets(\n",
        "    train_df,\n",
        "    test_df,\n",
        "    ner_model,\n",
        "    re_model,\n",
        "    enhance_function=enhance_with_special_characters,\n",
        "    average_type='micro',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ3-Omq5W8-P"
      },
      "source": [
        "# Dataset Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.866411900Z"
        },
        "id": "UmaY7j8dW8-P"
      },
      "outputs": [],
      "source": [
        "train_df=pd.read_csv('merged_train.tsv',sep='\\t')\n",
        "test_df=pd.read_csv('merged_test.tsv',sep='\\t')\n",
        "df=pd.concat([train_df,test_df])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.866411900Z"
        },
        "id": "QowOhAeGW8-P"
      },
      "outputs": [],
      "source": [
        "def add_problem(df, description, pattern=None, problems=None, contains=True, func=None):\n",
        "    if func:  # If a function is provided\n",
        "        df['is_problem'] = df.apply(func, axis=1)\n",
        "    else:\n",
        "        if contains:\n",
        "            df['is_problem'] = df['text'].str.contains(pattern, regex=True, na=False)\n",
        "        else:\n",
        "            df['is_problem'] = ~df['text'].str.contains(pattern, regex=True, na=False)\n",
        "    problem_df = df[df['is_problem'] == True]\n",
        "    problem = {\"description\": description, 'row_count': len(problem_df)}\n",
        "    print(f\"Problem: {description}. Number of examples affected: {len(problem_df)} out of {len(df)} ({round(100 * len(problem_df) / len(df), 2)}%)\")\n",
        "    print(\"**************************\")\n",
        "    problems.append(problem)\n",
        "    return problem_df\n",
        "\n",
        "def entity_mismatch_with_text_tag(row, entity_number):\n",
        "    tag = f\"<e{entity_number}>\"\n",
        "    closing_tag = f\"</e{entity_number}>\"\n",
        "    entity_key = f\"entity_{entity_number}\"\n",
        "    if isinstance(row[entity_key], str) and isinstance(row['text'], str):\n",
        "        parts = row['text'].split(tag)\n",
        "        if len(parts) > 1:\n",
        "            entity_in_text = parts[1].split(closing_tag)[0]\n",
        "            return row[entity_key].lower() not in entity_in_text.lower()\n",
        "    return False\n",
        "\n",
        "def is_tag_inside_word(row):\n",
        "    # Patterns to identify entity tags inside words\n",
        "    patterns = [r\"\\w<e1>\", r\"</e1>\\w\", r\"\\w<e2>\", r\"</e2>\\w\"]\n",
        "    text = row['text'] if isinstance(row['text'], str) else \"\"\n",
        "\n",
        "    # Check if any pattern is found in the text\n",
        "    return any(re.search(pattern, text) for pattern in patterns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.867547500Z"
        },
        "id": "-drh0rcMW8-P"
      },
      "outputs": [],
      "source": [
        "problems=[]\n",
        "problem_dfs=[]\n",
        "problem_dfs.append(add_problem(\n",
        "    df=df,\n",
        "    description=\"Does not contain correct entity tag pattern\",\n",
        "    pattern=r\"(<e1>.*</e1>.*<e2>.*</e2>)|(<e2>.*</e2>.*<e1>.*</e1>)\",\n",
        "    problems=problems,\n",
        "    contains=False\n",
        "))\n",
        "problem_dfs.append(add_problem(df=df, description=\"Entity mismatch with text tags\",\n",
        "func=lambda row: entity_mismatch_with_text_tag(row, 1) or entity_mismatch_with_text_tag(row, 2),\n",
        "problems=problems))\n",
        "problem_dfs.append(add_problem(df=df,description=\"Empty entity in text\",pattern=r'(.*<e1></e1>.*)|(.*<e2></e2>.*)', problems=problems))\n",
        "problem_dfs.append(add_problem(df=df,description=\"Multiple entities in text\",pattern=r\"(.*<e1>.*<e1>.*)|(.*<e2>.*<e2>.*)\",problems=problems))\n",
        "problem_dfs.append(add_problem(df=df, description=\"Overlapping entities\", pattern=r\"<e1>.*<e2>.*</e1>.*</e2>\", problems=problems))\n",
        "problem_dfs.append(add_problem(df=df, description=\"Missing entity_1 in text\", func=lambda row: isinstance(row['entity_1'], str) and isinstance(row['text'], str) and row['entity_1'].lower() not in row['text'].lower(), problems=problems))\n",
        "problem_dfs.append(add_problem(df=df, description=\"Missing entity_2 in text\", func=lambda row: isinstance(row['entity_2'], str) and isinstance(row['text'], str) and row['entity_2'].lower() not in row['text'].lower(), problems=problems))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.868497400Z"
        },
        "scrolled": true,
        "id": "ti0sdSgAW8-P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def df_percentage_contained(df1, df2):\n",
        "    \"\"\"Calculate the percentage of rows in df2 that are also in df1.\"\"\"\n",
        "    if len(df2) == 0:\n",
        "        return 0  # Avoid division by zero for empty dataframes\n",
        "    merged = df1.merge(df2, how='inner', indicator=True)\n",
        "    match_count = merged['_merge'].value_counts().get('both', 0)\n",
        "    return round((match_count / len(df2)) * 100)\n",
        "\n",
        "descriptions = [problem['description'] for problem in problems]\n",
        "percentage_matrix = []\n",
        "for df1 in problem_dfs:\n",
        "    row = []\n",
        "    for df2 in problem_dfs:\n",
        "        row.append(df_percentage_contained(df1, df2))\n",
        "    percentage_matrix.append(row)\n",
        "percentage_df = pd.DataFrame(percentage_matrix, columns=descriptions, index=descriptions)\n",
        "display(percentage_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYupntwfW8-P"
      },
      "outputs": [],
      "source": [
        "problem_dfs[1][problem_dfs[1]['lang']=='en'].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.869537900Z"
        },
        "id": "SLy6NuWUW8-Q"
      },
      "outputs": [],
      "source": [
        "all_problems_df = pd.concat(problem_dfs).drop_duplicates()\n",
        "unique_problems_ratio = len(all_problems_df) / len(df)\n",
        "unique_problems_ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDEYeeZzW8-Q"
      },
      "source": [
        "## Overlap analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-11-26T20:17:06.869537900Z"
        },
        "id": "t3sw94XnW8-Q"
      },
      "outputs": [],
      "source": [
        "full_matrix_all = create_full_matrix(\"results/train_overlap_all.csv\")\n",
        "full_matrix_distinct = create_full_matrix(\"results/train_overlap_distinct.csv\")\n",
        "display(full_matrix_all)\n",
        "display(full_matrix_distinct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4EdaIBiW8-Q"
      },
      "source": [
        "## Miscellaneous"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txtIPtyAW8-Q"
      },
      "source": [
        "### Dataset size graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJfTtneVW8-Q"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Data for the graphs (extracted from the provided table)\n",
        "dataset_sizes = [100, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000, 300000, 400000, 500000]\n",
        "\n",
        "# RE with tags\n",
        "re_with_tags_merged = [0.58, 0.82, 0.90, 0.92, 0.94, 0.94, 0.95, 0.96, 0.96, 0.96, 0.97, 0.97, 0.96]\n",
        "re_with_tags_en_full = [0.51, 0.85, 0.88, 0.90, 0.92, 0.95, 0.96, 0.96, 0.96, 0.96, 0.97, 0.96, 0.97]\n",
        "re_with_tags_pl = [0.46, 0.86, 0.91, 0.93, 0.97, 0.97] + [None] * 7\n",
        "\n",
        "# RE no tags\n",
        "re_no_tags_merged = [0.53, 0.68, 0.72, 0.73, 0.77, 0.77, 0.79, 0.82, 0.82, 0.84, 0.85, 0.85, 0.84]\n",
        "re_no_tags_en_full = [0.57, 0.68, 0.73, 0.73, 0.77, 0.79, 0.81, 0.83, 0.84, 0.85, 0.85, 0.85, 0.86]\n",
        "re_no_tags_pl = [0.58, 0.75, 0.77, 0.78, 0.83, 0.90] + [None] * 7\n",
        "\n",
        "# NER\n",
        "ner_merged = [0.50, 0.67, 0.71, 0.73, 0.77, 0.80, 0.81, 0.83, 0.84, 0.86, 0.86, 0.87, 0.88]\n",
        "ner_en_full = [0.52, 0.71, 0.74, 0.77, 0.79, 0.81, 0.81, 0.85, 0.84, 0.86, 0.87, 0.87, 0.88]\n",
        "ner_pl = [0.55, 0.71, 0.78, 0.82, 0.87, 0.86] + [None] * 7\n",
        "\n",
        "# Creating the Plotly graphs for each model type\n",
        "fig_re_with_tags = go.Figure()\n",
        "fig_re_no_tags = go.Figure()\n",
        "fig_ner = go.Figure()\n",
        "\n",
        "# RE with tags Graph\n",
        "for dataset, name in zip([re_with_tags_merged, re_with_tags_en_full, re_with_tags_pl],\n",
        "                         ['merged', 'en-full', 'pl']):\n",
        "    fig_re_with_tags.add_trace(go.Scatter(x=dataset_sizes, y=dataset, mode='lines+markers', name=name))\n",
        "\n",
        "# RE no tags Graph\n",
        "for dataset, name in zip([re_no_tags_merged, re_no_tags_en_full, re_no_tags_pl],\n",
        "                         ['merged', 'en-full', 'pl']):\n",
        "    fig_re_no_tags.add_trace(go.Scatter(x=dataset_sizes, y=dataset, mode='lines+markers', name=name))\n",
        "\n",
        "# NER Graph\n",
        "for dataset, name in zip([ner_merged, ner_en_full, ner_pl],\n",
        "                         ['merged', 'en-full', 'pl']):\n",
        "    fig_ner.add_trace(go.Scatter(x=dataset_sizes, y=dataset, mode='lines+markers', name=name))\n",
        "\n",
        "# Update layout for each graph for better visualization\n",
        "for fig, title in zip([fig_re_with_tags, fig_re_no_tags, fig_ner],\n",
        "                      ['Performance of RE with tags', 'Performance of RE without tags', 'Performance of NER']):\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title='dataset size',\n",
        "        yaxis_title='F1 ccore',\n",
        "        xaxis_type='log',  # Log scale for better visibility on large ranges of x\n",
        "        yaxis_range=[0, 1],  # F1 scores range from 0 to 1\n",
        "        legend_title='dataset type'\n",
        "    )\n",
        "\n",
        "# Show the figures\n",
        "fig_re_with_tags.show()\n",
        "fig_re_no_tags.show()\n",
        "fig_ner.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fJ87iZmW8-R"
      },
      "source": [
        "## Calculating baseline F1 values for merged train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdfZPlFpW8-R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def random_baseline_f1(labels):\n",
        "    label_counts = Counter(labels)\n",
        "    total_count = sum(label_counts.values())\n",
        "    label_probabilities = np.array(list(label_counts.values())) / total_count\n",
        "    random_predictions = np.random.choice(a=list(label_counts.keys()),\n",
        "                                          p=label_probabilities,\n",
        "                                          size=len(labels))\n",
        "    return f1_score(labels, random_predictions, average='weighted')\n",
        "\n",
        "def most_frequent_baseline_f1(labels):\n",
        "    most_common_label = Counter(labels).most_common(1)[0][0]\n",
        "    constant_predictions = [most_common_label] * len(labels)\n",
        "    return f1_score(labels, constant_predictions, average='weighted')\n",
        "\n",
        "train_df = pd.read_csv('merged_train.tsv', sep='\\t')\n",
        "test_df = pd.read_csv('merged_test.tsv', sep='\\t')\n",
        "train_labels = train_df['label']\n",
        "test_labels = test_df['label']\n",
        "train_random_f1 = random_baseline_f1(train_labels)\n",
        "train_most_frequent_f1 = most_frequent_baseline_f1(train_labels)\n",
        "test_random_f1 = random_baseline_f1(test_labels)\n",
        "test_most_frequent_f1 = most_frequent_baseline_f1(test_labels)\n",
        "print(f\"Train Random Baseline F1: {train_random_f1}\")\n",
        "print(f\"Train Most Frequent Baseline F1: {train_most_frequent_f1}\")\n",
        "print(f\"Test Random Baseline F1: {test_random_f1}\")\n",
        "print(f\"Test Most Frequent Baseline F1: {test_most_frequent_f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErrGEsVLW8-R"
      },
      "source": [
        "## Analyzing the labeled errors for NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM2xUHJcW8-R"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('results/error_analysis_ner.csv')\n",
        "correct_df=df[df['is_model_wrong']=='no']\n",
        "wrong_df=df[df['is_model_wrong']=='yes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcqF7mBpW8-R"
      },
      "outputs": [],
      "source": [
        "round(correct_df['issue'].value_counts(normalize=True) * 100,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x00vAdGvW8-R"
      },
      "outputs": [],
      "source": [
        "round(wrong_df['issue'].value_counts(normalize=True) * 100,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Mf4_NzW8-S"
      },
      "source": [
        "## Analyzing the labeled errors for RE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCs4a2FLW8-S"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('results/error_analysis_re.csv')\n",
        "correct_df=df[df['is_model_wrong']=='no']\n",
        "wrong_df=df[df['is_model_wrong']=='yes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQAjBC-2W8-S"
      },
      "outputs": [],
      "source": [
        "print(len(df),len(correct_df),len(wrong_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6lrXZpWW8-S"
      },
      "outputs": [],
      "source": [
        "print(len(wrong_df[wrong_df[\"predicted_label\"]==\"birth-place\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "BHGHbcEwW8-S"
      },
      "outputs": [],
      "source": [
        "wrong_df['predicted_label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLWjqlfJW8-S"
      },
      "outputs": [],
      "source": [
        "wrong_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFwoICTxW8-T"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}